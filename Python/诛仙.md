---
title: '梦回诛仙之爬虫'
date: 2021-07-13 12:52
categories:
- Python
tags:
- 爬虫
---

## 前因
起因是小时看的诛仙，直觉热血沸腾，久久不能相忘，故今日购置了全套诛仙典藏版实体书再阅，看至近结尾处才发觉书有大量修订删减，于脑海之印象不能重合，遂百度限制时间查找以寻原版诛仙下载。
<!-- more -->
搜寻期间，多数网站不是崩溃即是改换门庭，让人不甚唏嘘。同时也找到了数站提供txt下载，下载链接却都已无法响应，仍保留存活的网站也已更新为最新修订版，令人唏嘘。经过搜寻，幸得一在线阅读网站——努努书坊，不得不说，努努书坊真的可以，在其站内还可看到当年流行的UI布局设计，存在了十几年还在服务器上跑着，没有崩掉，这也给了我一次找到原版无修诛仙的机会。https://www.kanunu8.com/files/yuanchuang/201102/1530.html

可惜之处在于，这个老网站随提供部分小说txt下载功能，诛仙却无法下载，只可在线阅读，略略阅读过删减章节后，令我激动不已。遂想到使用爬虫爬取整合为txt，故有本文下篇。

## 后果（爬虫）
有了任务目标，且有了地址，一切就简单了起来。
首先简要步骤如下：
> 1、获取小说主页源代码
> 2、在主页源代码中找到每个章节的超链接
> 3、获取每个章节超链接的源代码
> 4、获取章节的内容
> 5、保存内容到本地

小说站点诛仙的url是{% link 努努书坊 https://www.kanunu8.com/files/yuanchuang/201102/1530.html %}
### 1.获取小说主页源代码
首先导入模板

{% codeblock lang:Python %}
import re
import urllib.request
{% endcodeblock %}

然后定义一个函数，专门用来爬取网站小说的

{% codeblock lang:Python %}
import re
import urllib.request
#定义一个爬取网络小说的函数
def get_novel():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
{% endcodeblock %}

最后一行我们调用了urllib库的方法，urlopen方法中我们传进一个网址作为参数表示我们需要爬取的网站，.read()方法表示获取源代码。那我们现在打印html是否能成功在控制台把页面的代码给输出了呢？答案是否定的，现在获取的源码是一个乱码，我们还需要对该代码进行转码，于是要在下面加多一行转码的。

{% codeblock lang:Python %}
import re
import urllib.request
#定义一个爬取网络小说的函数
def get_novel():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
    html = html.decode("gbk")  # 转成该网址的格式
{% endcodeblock %}

由上面我们可知代码已经转成了‘gbk’格式，并且也已经将它存在html这个变量上了，那我们怎么知道转成什么格式呢？
前面我说过我要爬取的网站是：https://www.kanunu8.com/files/yuanchuang/201102/1530.html
![jpg](../诛仙/novel1.png)
我建议小伙伴们在获取该网页源代码的时候先在浏览器中查看分析一下该网页源代码的大概结构，分析清楚了好方便后面代码的编写。
### 2、在主页源代码中找到每个章节的超链接
在浏览器中查看源代码
![jpg](../诛仙/novel2.png)

回到编辑器这边把刚才的代码粘贴过来并打上注释，作为一个参考的模板
因为我们需要抓取的是全部章节而不仅仅只是这一个章节，所以我们要用到正则表达式来进行匹配，先把通用的部分用(.*?)替代，(.*?)可以匹配所有东西

{% codeblock lang:Python %}
import re
import urllib.request
import os
#定义一个爬取网络小说的函数
def get_novel():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
    html = html.decode("gbk")  # 转成该网址的格式
    #<td width="25%"><a href="1530/35120.html">序章</a></td>
    reg = r'<td><a href="(.*?)">(.*?)</a></td>'  #正则表达的匹配
    reg = re.compile(reg)  #可添加可不添加，增加效率
    urls = re.findall(reg, html)
    print(urls)

get_novel()
{% endcodeblock %}
仔细的小伙伴就发现有些地方的.*?加括号，有些地方又不加，这是因为加了括号的都是我们要匹配的，不加括号是我们不需要匹配的。接下来一行调用re.compiled()方法是增加匹配的效率，建议习惯加上，最后一行开始与我们一开始获取的整个网页的源代码进行匹配。到这步我们已经能把代码所有章节以及章节链接的代码都获取了，打印在控制台上看一下
![jpg](../诛仙/novel3.png)

从结果我们可以知道对于每个循环出来的结果为一个元祖，有两个索引，0和1，0为章节的超链接，1为章节的名字。
既然我们获取到了每个章节的超链接那我们是不是就可以像上面的操作步骤一样获取这个链接里面的内容啦？答案是肯定的。
### 3、获取每个章节超链接的源代码
于是我们继续写以下的代码获取每个章节的代码(跟上面获取主页代码的步骤类似)
{% codeblock lang:Python %}
#定义一个爬取网络小说的函数
def getNovelContent():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
    html = html.decode("gbk")   #转成该网址的格式
    #  <td width="25%"><a href="1530/35120.html">序章</a></td>
    reg = r'<td><a href="(.*?)">(.*?)</a></td>'  # 正则表达的匹配
    reg = re.compile(reg)     #可添加可不添加，增加效率
    urls = re.findall(reg,html)
    # print(urls)
    for url in urls:
        chapter_url = "https://www.kanunu8.com/files/yuanchuang/201102/" + url[0]  #章节的超链接，根据你获取到的链接形式定义
        chapter_title = url[1]  #章节的名字
        print(chapter_url,chapter_title)
        chapter_html = urllib.request.urlopen(chapter_url).read()   #正文内容源代码
        chapter_html = chapter_html.decode("gbk")

getNovelContent()
{% endcodeblock %}

至此我们已经可以得到每个章节的内容了，但是还有一个问题，就是我们只想获取正文内容，正文以外的东西我们统统不要。
### 4、获取章节的内容
所以我们可以在网页中打开其中一章并查看源代码，例如我打开的是第一章，查找正文的内容在哪个标签中。
![jpg](../诛仙/novel4.png)
上图标识了我找到的正文内容开始与结束标签的位置，标签为<p></p>
于是我们在代码中又可以通过正则表达式来匹配内容了
{% codeblock lang:Python %}
#定义一个爬取网络小说的函数
def getNovelContent():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
    html = html.decode("gbk")   #转成该网址的格式
    #  <td width="25%"><a href="1530/35120.html">序章</a></td>
    reg = r'<td><a href="(.*?)">(.*?)</a></td>'  # 正则表达的匹配
    reg = re.compile(reg)     #可添加可不添加，增加效率
    urls = re.findall(reg,html)
    # print(urls)
    for url in urls:
        chapter_url = "https://www.kanunu8.com/files/yuanchuang/201102/" + url[0]  #章节的超链接
        chapter_title = url[1]  #章节的名字
        print(chapter_url,chapter_title)
        chapter_html = urllib.request.urlopen(chapter_url).read()   #正文内容源代码
        chapter_html = chapter_html.decode("gbk")
        chapter_reg = r'<p>.*?</p>'
        chapter_reg = re.compile(chapter_reg, re.S)
        chapter_content = re.findall(chapter_reg, chapter_html)
        for content in chapter_content:  # 打印章节的内容
            content = content.replace("<br />", "")  # 把"<br/>"字符全部替换为""
            content = content.replace("<p>", "")  # 把"<p>"字符全部替换为""
            content = content.replace("</p>", "")  # 把"<p>"字符全部替换为""
            print(content)  # 打印内容

getNovelContent()
{% endcodeblock %}
![jpg](../诛仙/novel5.png)
走到此处已令人激动了，我们已经成功将文章爬取下来，剩下的问题就是优化和存储了。
### 5、保存内容到本地
保存方式有两种，一种是保存到本地，另一种是保存到数据库中，这次我选择了保存到本地。
{% fold 点我折叠 %}
{% codeblock lang:Python %}
def getNovelContent():
    html = urllib.request.urlopen("https://www.kanunu8.com/files/yuanchuang/201102/1530.html").read()
    html = html.decode("gbk")   #转成该网址的格式
    #  <td width="25%"><a href="1530/35120.html">序章</a></td>
    reg = r'<td><a href="(.*?)">(.*?)</a></td>'  # 正则表达的匹配
    reg = re.compile(reg)     #可添加可不添加，增加效率
    urls = re.findall(reg,html)
    # print(urls)
    i = 4
    for url in urls:
        chapter_url = "https://www.kanunu8.com/files/yuanchuang/201102/" + url[0]  #章节的超链接
        chapter_title = url[1]  #章节的名字
        print(chapter_url,chapter_title)
        chapter_html = urllib.request.urlopen(chapter_url).read()   #正文内容源代码
        try:
            chapter_html = chapter_html.decode("gbk")
        except: # 注1
            print('%s下载失败' % chapter_title)
            i = i + 1
            continue
        chapter_reg = r'<p>.*?</p>'
        chapter_reg = re.compile(chapter_reg, re.S)
        chapter_content = re.findall(chapter_reg, chapter_html)
        for content in chapter_content:  # 打印章节的内容
            # content = content.replace("&nbsp;&nbsp;&nbsp;&nbsp", "")  # 把"&nbsp;"字符全都替换为""
            content = content.replace("<br />", "")  # 把"<br/>"字符全部替换为""
            content = content.replace("<p>", "")  # 把"<p>"字符全部替换为""
            content = content.replace("</p>", "")  # 把"<p>"字符全部替换为""
            # print(content)  # 打印内容
            base_path = 'H:/BDml/01/zhuxian'
            novel_path = os.path.join(base_path, str(i)+chapter_title+".txt")
            i = i+1
            with open(novel_path, 'w') as f:
                f.write(content)
                print('%s下载成功' % chapter_title)

getNovelContent()
{% endcodeblock %}
{% endfold %}
![jpg](../诛仙/novel6.png)
![jpg](../诛仙/novel7.png)
![jpg](../诛仙/novel8.png)

仍有许多值得优化之处，因呜呜呜辛苦二战无法继续，遂留至有缘再续
### 注1
{% codeblock lang:Python %}
try:
    chapter_html = chapter_html.decode("gbk")
except: # 注1
    print('%s下载失败' % chapter_title)
    i = i + 1
    continue
{% endcodeblock %}
加此原因是老网站编码格式也过于腐朽，某些章节爬取时会遇到编码格式问题而出现编码错误
网站编码是gb2312,本文采用转为gbk编码，尝试utf-8等格式转译均不可，遂暂时放弃
![jpg](../诛仙/novel9.png)

参考：https://www.cnblogs.com/zhangguosheng1121/p/11312533.html
